> library(faraway) 
> library(psych)
> 
> data(fat, package="faraway")
> describe(fat)
        vars   n   mean    sd median trimmed   mad   min    max  range  skew kurtosis   se
brozek     1 252  18.94  7.75  19.00   18.84  8.45   0.0  45.10  45.10  0.14    -0.35 0.49
siri       2 252  19.15  8.37  19.20   19.05  9.27   0.0  47.50  47.50  0.14    -0.37 0.53
density    3 252   1.06  0.02   1.05    1.06  0.02   1.0   1.11   0.11 -0.02    -0.35 0.00
age        4 252  44.88 12.60  43.00   44.44 11.86  22.0  81.00  59.00  0.28    -0.45 0.79
weight     5 252 178.92 29.39 176.50  177.41 28.73 118.5 363.15 244.65  1.19     5.08 1.85
height     6 252  70.15  3.66  70.00   70.27  2.97  29.5  77.75  48.25 -5.32    57.86 0.23
adipos     7 252  25.44  3.65  25.05   25.17  3.11  18.1  48.90  30.80  1.54     6.48 0.23
free       8 252 143.71 18.23 141.55  142.69 16.46 105.9 240.50 134.60  0.95     2.54 1.15
neck       9 252  37.99  2.43  38.00   37.96  2.37  31.1  51.20  20.10  0.55     2.60 0.15
chest     10 252 100.82  8.43  99.65  100.28  8.38  79.3 136.20  56.90  0.67     0.91 0.53
abdom     11 252  92.56 10.78  90.95   92.00 10.90  69.4 148.10  78.70  0.83     2.14 0.68
hip       12 252  99.90  7.16  99.30   99.49  5.78  85.0 147.70  62.70  1.48     7.22 0.45
thigh     13 252  59.41  5.25  59.00   59.17  4.60  47.2  87.30  40.10  0.81     2.55 0.33
knee      14 252  38.59  2.41  38.50   38.50  2.22  33.0  49.10  16.10  0.51     0.99 0.15
ankle     15 252  23.10  1.69  22.80   22.98  1.33  19.1  33.90  14.80  2.23    11.57 0.11
biceps    16 252  32.27  3.02  32.05   32.24  2.89  24.8  45.00  20.20  0.28     0.44 0.19
forearm   17 252  28.66  2.02  28.70   28.68  2.08  21.0  34.90  13.90 -0.22     0.80 0.13
wrist     18 252  18.23  0.93  18.30   18.21  0.89  15.8  21.40   5.60  0.28     0.34 0.06
> 
> X1 <- scale(fat$abdom)  
> X2 <- scale(fat$weight) 
> X3 <- scale(fat$wrist)
> n <- 252
> 
> set.seed(12534)
> error <- rnorm(n, m=0, sd=1)
> 
> for(i in 1:n)
+ {
+ y[i] <- 0  
+ }
> 
> y <- scale(2*X1 + 3*X2 + 4*X3 + error)
> head(y)
           [,1]
[1,] -1.3772855
[2,] -0.1694829
[3,] -1.4526843
[4,]  0.1167905
[5,] -0.1998786
[6,]  0.6702376
> 
> newdata <- cbind(y,X1,X2,X3)
> describe(newdata)
   vars   n mean sd median trimmed  mad   min  max range skew kurtosis   se
X1    1 252    0  1  -0.05   -0.03 0.95 -2.22 5.07  7.29 0.71     2.18 0.06
X2    2 252    0  1  -0.15   -0.05 1.01 -2.15 5.15  7.30 0.83     2.14 0.06
X3    3 252    0  1  -0.08   -0.05 0.98 -2.06 6.27  8.32 1.19     5.08 0.06
X4    4 252    0  1   0.08   -0.02 0.95 -2.60 3.40  6.00 0.28     0.34 0.06
> 
> lmod <- lm(y ~ 0 + X1 + X2 + X3) #note the 0 indicates regression through the origin
> lmod

Call:
lm(formula = y ~ 0 + X1 + X2 + X3)

Coefficients:
    X1      X2      X3  
0.2285  0.3758  0.4858  

> 
> confint(lmod)
       2.5 %    97.5 %
X1 0.1962577 0.2607117
X2 0.3388094 0.4127895
X3 0.4641398 0.5074920
> 
> ##------Bootstrapping Confidence Intervals in Linear Regression--------##
> ##--this example bootstraps OLS Regression where n=# observations and p=# of parameters and k=#X's ##
> ##bootstrapping process
> # fit the desired linear model (lmod) of y on X1, X2, . . ., Xk and obtain Bhat - want bootstrapped CIs on B's
> # obtain the residuals e1, e2, . . ., en  Fhat of es is a good estimator of F of the population of error terms
> # take a random sample with replacement of n residuals from e1, e2, . . ., en and label as e*1, e*2, . . ., e*n
> # generate new observations yhat*1, yhat*2, . . .,yhat*n
> # where yhat*i = the ith row vector of X multiplied by Bhat column vector + e*i  i=1, 2, . . . ,n
> # keep in mind the ith row vector of the X matrix = covariate observations on the ith experimental unit
> # now run the same desired linear model of yhat* on X1, X2, . . ., Xk to obtain Bhat*1, Bhat*2, ..., Bhat*p
> # keep track of the Bhats from each run 1, 2, . . ., nb where nb = # bootstrap runs is at least 200
> # the std error of Bhati is equal to the std deviation of the bootstrapped Bhat*i's - there are nb of them 
> # now form the 95% confidence interval:
> # Bhati +/- 1.96 x (std deviation of the bootstrapped Bhat*i's)
> #---------------------------------------------------------------------------------------------------------#
> 
> sample(n,rep=TRUE)
  [1] 209 162  19 229 166 120 121  14 188 151 175 165  54 123 187 175   2 168 175 138 234  94  40 162   6 156  24 164  89  70
 [31] 125  44 224 232 133  72  89 126  89  91 206  63  18 129 191 165 246 203 169 108  93 169 121  30 105  52 155  40 215 198
 [61]  24  85  46 197  50  27  20 201 241 202  87  53 243  14  66  19  97  21 245 178  34 186   1   5 246  68  86 125  69  52
 [91] 165  56 225  76  15  70 229  50 241 241  73 250 128 217 192 108 215 197  20 146  76 156  47  77  74 148 148  19 157 235
[121] 184 111 103  56 171  85   1  34 247  45 105 108 134  64 226  70 114 112 139  57 195  43 240  36  52 190  93  12  25  25
[151] 205 245  53 234  21 132   8 211 114  29 249 100  73   9 132 221 113  63  69 211 176  81  41  14   1  97 106  56 143 188
[181] 220  94  82  47 204   6  20  81 201  84  68 231 134 198 237 219   4  94  72  31  22 218  54 207 192 181 203 131  24 170
[211]  84  79  22 200 230 220  85  22  50  52 152 252  15  12 119  44  95 180 190 143   5 238 132  81 182 113 190 197  22 135
[241]  99   1 246 151  63  95  72 221 219 181   7 118
> ##takes a sample of size 252 WITH REPLACEMENT from the residuals.  creates the indices of the sampled residuals##
> ##should try to take a sample as large as the number of observations in the data set##
> 
> set.seed(123)
> nb <- 4000
> coefmat <- matrix(NA,nb,3) #sets up (nb by 3) matrix to keep track of the results of each bootstrap
> head(coefmat)
     [,1] [,2] [,3]
[1,]   NA   NA   NA
[2,]   NA   NA   NA
[3,]   NA   NA   NA
[4,]   NA   NA   NA
[5,]   NA   NA   NA
[6,]   NA   NA   NA
> 
> resids <- residuals(lmod) #store the e1, e2, . . ., en from the desired linear model
> resids
           1            2            3            4            5            6            7            8            9 
-0.318005099  0.121046140 -0.187229588  0.188225642 -0.150034572 -0.066135596 -0.150174089 -0.184090718  0.053589940 
          10           11           12           13           14           15           16           17           18 
-0.051031070 -0.020506078  0.060346766 -0.025641680  0.195762950  0.060501633 -0.105381679  0.093432197 -0.093523410 
          19           20           21           22           23           24           25           26           27 
 0.054424239  0.185156450  0.005773374 -0.149672722  0.092595460 -0.147618830 -0.197331330  0.159417984  0.088628463 
          28           29           30           31           32           33           34           35           36 
-0.054856926  0.029368959 -0.042180929 -0.259288013 -0.147174484  0.242107395 -0.068013818  0.038787930  0.038285189 
          37           38           39           40           41           42           43           44           45 
-0.044781333  0.126388420 -0.116221648 -0.212115891  0.166340800 -0.083348225 -0.135327634  0.105077378  0.072095430 
          46           47           48           49           50           51           52           53           54 
-0.155455881  0.088070584 -0.058794761 -0.004660676  0.053024567  0.175822217  0.251339669  0.143197269  0.008630470 
          55           56           57           58           59           60           61           62           63 
 0.121745188 -0.096945911  0.065863314  0.041265483 -0.002019291  0.037001387  0.100362620  0.138531420  0.046952908 
          64           65           66           67           68           69           70           71           72 
 0.067430684 -0.115920243  0.016128982 -0.113582223  0.080750937 -0.071223198  0.054400157  0.076460757 -0.044194527 
          73           74           75           76           77           78           79           80           81 
 0.004476064 -0.153412735  0.174524277  0.020198005 -0.139952881 -0.161647571 -0.191400580  0.099494261  0.223774773 
          82           83           84           85           86           87           88           89           90 
-0.027182160 -0.086970688 -0.128351056 -0.037946511 -0.016893615 -0.012313640 -0.020642372 -0.037769662 -0.169263189 
          91           92           93           94           95           96           97           98           99 
-0.024895890 -0.100274492  0.130371254 -0.133714341  0.013966807  0.115651514 -0.036923872 -0.049724491 -0.005435267 
         100          101          102          103          104          105          106          107          108 
 0.009638417 -0.119713614 -0.026028332  0.077053582 -0.061471429  0.124526839  0.163387399 -0.056743821 -0.144224466 
         109          110          111          112          113          114          115          116          117 
-0.220161503 -0.037067893 -0.078969177  0.067204739 -0.145951866  0.020324024  0.033599763 -0.072842057 -0.105676883 
         118          119          120          121          122          123          124          125          126 
-0.269195469  0.204972177 -0.018493900 -0.058225310 -0.160155747 -0.056017816  0.180095941  0.049636940 -0.140560624 
         127          128          129          130          131          132          133          134          135 
 0.049127461 -0.045284829 -0.128922477  0.134330255  0.118318163 -0.016679350 -0.068532424  0.113638705 -0.064723293 
         136          137          138          139          140          141          142          143          144 
-0.081959782  0.020202909 -0.020832067 -0.067098447 -0.135275398 -0.133844460 -0.099904740  0.052952002  0.140357097 
         145          146          147          148          149          150          151          152          153 
-0.032684819 -0.234029297 -0.098377960  0.161656315  0.154026324  0.079424117  0.183631836 -0.003078853  0.090785787 
         154          155          156          157          158          159          160          161          162 
-0.002036841  0.043986706 -0.094117262 -0.044932341 -0.128843134  0.109616827  0.040524938  0.147994112  0.162984105 
         163          164          165          166          167          168          169          170          171 
-0.046026717 -0.138235456  0.031031721 -0.192468736 -0.003068155 -0.026259772  0.092383298  0.137960056 -0.249992874 
         172          173          174          175          176          177          178          179          180 
-0.067134961  0.025467145  0.053946286 -0.146952125  0.048685618  0.043007952 -0.087343599  0.102360855 -0.047676545 
         181          182          183          184          185          186          187          188          189 
 0.071146390 -0.058603895 -0.303343732  0.220914415 -0.255138089  0.229854899 -0.007776664  0.171757539  0.230239849 
         190          191          192          193          194          195          196          197          198 
-0.040652049 -0.046784116  0.023704825 -0.090298175  0.188652584 -0.001450842  0.005663262  0.066184442 -0.068414324 
         199          200          201          202          203          204          205          206          207 
-0.003780960  0.024826878  0.064168578 -0.066244003 -0.024915464 -0.017894747  0.312264057  0.122166265  0.031106910 
         208          209          210          211          212          213          214          215          216 
 0.108296998  0.059488230 -0.122014681  0.113484745 -0.009473536  0.046550652  0.069882349 -0.053836378 -0.226640795 
         217          218          219          220          221          222          223          224          225 
 0.327057508 -0.226594957 -0.050610535  0.078234509  0.004138650  0.082912841 -0.043511730  0.048513080  0.057665028 
         226          227          228          229          230          231          232          233          234 
 0.110801822  0.020502767  0.056544381 -0.149575276  0.117272740 -0.002107828 -0.009372397  0.042449195  0.074398305 
         235          236          237          238          239          240          241          242          243 
-0.189403960  0.125398433  0.075539657 -0.049223314  0.025566567 -0.107018016 -0.115067508  0.144376730 -0.020565124 
         244          245          246          247          248          249          250          251          252 
 0.257116219 -0.085954141 -0.002021591  0.015810167 -0.018279047  0.020097585  0.035801587  0.156715139  0.118686083 
attr(,"scaled:center")
[1] 0.08167823
attr(,"scaled:scale")
[1] 8.128303
> 
> preds <- fitted(lmod) #store the yhat (fitted values) from the desired linear model
> preds
            1             2             3             4             5             6             7             8 
-1.0592804337 -0.2905290064 -1.2654547617 -0.0714351399 -0.0498439969  0.7363732315 -0.2884612880  0.1734019219 
            9            10            11            12            13            14            15            16 
-0.0741541930  0.6681825143  0.0445286692  0.8398117125 -0.2757845172  0.8292382277  0.1788178352 -0.8936283370 
           17            18            19            20            21            22            23            24 
-0.1872252153  1.0494611280  0.1396963154  0.5725817513  0.1604122692  1.2773468340 -1.6329141962 -1.2397927965 
           25            26            27            28            29            30            31            32 
-1.0260368929 -0.7996598778 -1.8870164367 -1.1170747389 -1.5152078449 -0.7519981043  0.0462110939 -0.5810893730 
           33            34            35            36            37            38            39            40 
-0.1280725817  0.9190824642  2.1270426195 -0.0406267395  0.8758822859  0.9692071483  5.1823463663  0.6737709829 
           41            42            43            44            45            46            47            48 
 3.4344872905  0.1504869377  0.9705132939  0.9685235259 -2.1454210920 -0.3853581998 -1.4642112512 -0.8372827035 
           49            50            51            52            53            54            55            56 
-1.7502820947 -1.8710460501 -0.8722732446 -1.3537290113 -1.4485403718 -0.4685929135 -0.7219569526  0.7504295417 
           57            58            59            60            61            62            63            64 
 0.3270553319  1.0370949803  0.5560100526  0.2728942739  1.7587936842  0.0797508029  0.8446206769  0.4362949217 
           65            66            67            68            69            70            71            72 
 0.7026842794  0.4672993081 -0.9658776209 -0.1906191711 -0.6702031459 -0.9359510673  0.0242045686 -0.5815849052 
           73            74            75            76            77            78            79            80 
-0.4524855392 -1.7323164016 -1.3995067968 -0.5835935864 -0.5690216523  0.3335089539 -0.1291894194  0.4307498570 
           81            82            83            84            85            86            87            88 
 0.2755175436 -0.7176834477  0.6567516733  0.4500316924 -0.1057576921 -0.2284802079 -0.0684366714 -0.3445999179 
           89            90            91            92            93            94            95            96 
 0.1300191917  0.0789858520 -0.2357822697  0.3209424214 -0.2866307988  0.7312033656 -0.2351252983  1.8528991311 
           97            98            99           100           101           102           103           104 
 0.2460097503 -0.3495893331 -0.3945275953  0.2982812819  0.4947640313 -0.7178849128 -0.0614741426  1.0553958465 
          105           106           107           108           109           110           111           112 
-0.1722146853  0.0137112930  0.4222286148  0.9555394116  0.6159566453 -0.6225469109 -0.2134963290 -0.1127980730 
          113           114           115           116           117           118           119           120 
 0.8602792618 -0.6980486063 -0.4300403196 -0.8775889164 -0.1430942870  0.6078105036  0.2917318765 -0.1170381114 
          121           122           123           124           125           126           127           128 
 1.1543938449  0.2869473044 -0.7626294478 -0.8151860317 -0.5279371138 -0.5364684510 -0.5637393487 -1.0813036134 
          129           130           131           132           133           134           135           136 
 0.1228730307 -0.5682567014 -0.4714923190 -0.7338363863  0.2130278903 -0.8310461800 -0.3175071139 -0.0561962971 
          137           138           139           140           141           142           143           144 
-0.8803725452 -0.0347543379 -0.5501495624  0.9748112503 -0.8643664927 -0.2867877798 -0.3570472957 -0.5881698781 
          145           146           147           148           149           150           151           152 
 0.0007702569 -1.0027521500  0.7615475009  0.6095253145 -1.4819653843  0.9540028912 -0.8816285700  1.7355736154 
          153           154           155           156           157           158           159           160 
-1.2714062330 -0.5474438840  1.0415076333 -0.5625000766  1.0595501966  0.2011772681 -1.5725522805 -0.0402025391 
          161           162           163           164           165           166           167           168 
-1.4946243221  0.7356984272  0.8235149709 -1.5918619455  0.9347444272  1.1497440611 -0.4834264301  1.5692968351 
          169           170           171           172           173           174           175           176 
 1.4221070827 -0.5479482342 -0.8425527127 -1.7439153396 -0.3448882499 -0.3056375786  1.9289731305 -1.4013362058 
          177           178           179           180           181           182           183           184 
-1.3513673241  1.2615279883  0.0419866296  1.8360803047  1.1716614607 -2.1634313675 -1.1498350421 -0.8053296358 
          185           186           187           188           189           190           191           192 
-0.4290816415 -0.3492218722  1.9642521019  0.9910344014  0.7103582603  0.4014747363 -0.7564330484  2.1524974829 
          193           194           195           196           197           198           199           200 
 0.4637963910  1.0051097231 -0.9421151788  0.3961824360 -0.8606420188 -0.8921830433 -0.3170839938  0.1316971581 
          201           202           203           204           205           206           207           208 
 0.1129297680 -0.9469175838  0.7440082076  0.0802607899  0.5840693709  1.0696782701 -0.6206117219  0.4794260661 
          209           210           211           212           213           214           215           216 
-0.6946814983 -0.7457148381 -1.2018860971  1.0685492363 -0.0043457253  0.7100553751 -0.3035275756  1.2270495353 
          217           218           219           220           221           222           223           224 
-0.8458274154 -0.3539184972  0.9225263441 -0.9819214997  0.0044594889  1.7442452199 -0.5482600359 -1.3156162903 
          225           226           227           228           229           230           231           232 
 0.5735904518 -2.2389341431  0.1453782695  0.4267103017  0.1732916836  0.2673358344 -1.2210443196  0.5311474865 
          233           234           235           236           237           238           239           240 
-0.2577687033 -0.4976542746 -0.2184412601 -0.0290490124  0.6812111125  1.1051349502 -0.4728953281  0.7596066750 
          241           242           243           244           245           246           247           248 
-1.8300995795  1.7308791899  1.7168874554  1.7270043536  0.6907134222 -0.3338394230  1.0832024004 -0.6203958019 
          249           250           251           252 
 1.5191887069  0.3819123069  1.1536088023  2.0927678903 
attr(,"scaled:center")
[1] 0.08167823
attr(,"scaled:scale")
[1] 8.128303
> 
> for(i in 1:nb)
+ {
+ 
+ #this row below resamples residuals to obtain the new predicted observations
+ booty <- preds + sample(resids, rep=TRUE) #sampling with replacement
+ bmod <- update(lmod, booty ~ 0 + X1 + X2 + X3)
+ coefmat[i,] <- coef(bmod)
+ 
+ }
> 
> coefmat <- data.frame(coefmat)
> head(coefmat)
         X1        X2        X3
1 0.1946935 0.4061593 0.4821711
2 0.2256583 0.3634441 0.4952055
3 0.2105824 0.3943133 0.4809414
4 0.2393852 0.3690759 0.4764714
5 0.2432854 0.3758732 0.4714671
6 0.2245722 0.3804638 0.4838106
> apply(coefmat,2,function(x) quantile(x,c(0.025,0.975)))
             X1        X2        X3
2.5%  0.1974066 0.3390369 0.4643788
97.5% 0.2604211 0.4116361 0.5071216
> #first position of apply( , , ,) is an array or matrix, second position=2 indicates to operate on columns
> #third position is the function to apply
> 
> #compare to the CIs obtained by usual methods
> confint(lmod)
       2.5 %    97.5 %
X1 0.1962577 0.2607117
X2 0.3388094 0.4127895
X3 0.4641398 0.5074920
> 
> windows(7,7)
> #save graph in pdf
>  pdf(file="C:/Users/jmard/Desktop/RegressionMethodsSpring2020/Backup/bootsimOLSR_out.pdf")
> 
> #show a distribution of the bootstrapped Betahat's for x1 and x2 and x3
> 
> #install.packages("ggplot2")  #install if needed
> #show bootstrapped 95% CIs for x1 and x2 and x3
> 
> require(ggplot2)
> 
> ggplot(coefmat, aes(x=X1)) + geom_density() + geom_vline(xintercept=c(0.197,0.260),lty=2)
> 
> ggplot(coefmat, aes(x=X2)) + geom_density() + geom_vline(xintercept=c(0.339,0.411),lty=2)
> 
> ggplot(coefmat, aes(x=X3)) + geom_density() + geom_vline(xintercept=c(0.464,0.507),lty=2)
> 
> ##------------------------------##
> 
> dev.off()
windows 
      2 
> dev.off()
null device 
          1 
> 
